{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c75c6a-b8bc-4cd9-8a7f-9e63fb8b5ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive namespace is empty.\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70d098-65c3-4a99-813e-df97823ceac6",
   "metadata": {},
   "source": [
    "add nn.DataParallel and double check that the data obtained in dataloader for concat and sep are actually what you think they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f45e3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import esm\n",
    "from os.path import exists\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "from torch.utils.data import Dataset\n",
    "import warnings\n",
    "import seaborn\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "#from transformers import EsmTokenizer, EsmModel\n",
    "from torch.utils.data import dataset\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fce5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "875c8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peptide_10mer = pd.read_csv(\"MS10_dat_dummy_seed_757_5cv_x10p_train_test_valid.txt\", sep = \" \")\n",
    "peptide_10mer_subset = pd.read_csv(\"MS10_dat_dummy_seed_757_5cv_x10p_train_test_valid.txt\", sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de44734d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'samples_per_fold_and_set = []\\n\\n# Iterate through each fold and set type to sample 5 rows each\\nfor fold in range(5):  # Folds 0 through 4\\n    for set_type in [\\'train\\', \\'valid\\']:  # Assuming you meant \\'val\\' instead of \\'valid\\' as per your requirement\\n        # Sample 5 rows where the fold and set type match the current iteration\\n        samples = peptide_10mer[(peptide_10mer[\\'fold\\'] == fold) & (peptide_10mer[\\'set\\'] == set_type)].sample(n=5)\\n        samples_per_fold_and_set.append(samples)\\n\\n# Concatenate all the sampled DataFrames\\npeptide_10mer_subset = pd.concat(samples_per_fold_and_set, ignore_index=True)\\n\\n# Optional: Filter the columns if necessary (seems you want to keep all relevant columns)\\npeptide_10mer_subset = peptide_10mer_subset[[\"allele\", \"fold\", \"set\", \"seq\", \"label\"]]\\n\\n# Display the counts to verify the distribution\\nprint(peptide_10mer_subset[[\"fold\", \"set\"]].value_counts())'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''samples_per_fold_and_set = []\n",
    "\n",
    "# Iterate through each fold and set type to sample 5 rows each\n",
    "for fold in range(5):  # Folds 0 through 4\n",
    "    for set_type in ['train', 'valid']:  # Assuming you meant 'val' instead of 'valid' as per your requirement\n",
    "        # Sample 5 rows where the fold and set type match the current iteration\n",
    "        samples = peptide_10mer[(peptide_10mer['fold'] == fold) & (peptide_10mer['set'] == set_type)].sample(n=5)\n",
    "        samples_per_fold_and_set.append(samples)\n",
    "\n",
    "# Concatenate all the sampled DataFrames\n",
    "peptide_10mer_subset = pd.concat(samples_per_fold_and_set, ignore_index=True)\n",
    "\n",
    "# Optional: Filter the columns if necessary (seems you want to keep all relevant columns)\n",
    "peptide_10mer_subset = peptide_10mer_subset[[\"allele\", \"fold\", \"set\", \"seq\", \"label\"]]\n",
    "\n",
    "# Display the counts to verify the distribution\n",
    "print(peptide_10mer_subset[[\"fold\", \"set\"]].value_counts())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc41d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.read_csv(\"aafeatmat_onehot_ext.txt\", sep = \" \")\n",
    "one_hot = one_hot.rename(index={'.': 'B'}, columns = {'.': 'B'})\n",
    "one_hot_dict = {index: row.tolist() for index, row in one_hot.iterrows()}\n",
    "\n",
    "kf = pd.read_csv(\"aafeatmat_KideraFactors.txt\", sep = \" \")\n",
    "\n",
    "pc = pd.read_csv(\"aafeatmat_AAPropsPCA3.txt\", sep = \" \")\n",
    "pc.loc[\"B\"] = 0\n",
    "pc_dict = {index: row.tolist() for index, row in pc.iterrows()}\n",
    "\n",
    "hla_full_pc_kf_pseudoseq_wseq = pd.read_csv(\"ABCG_prot.parsed.clean.updated.ALL.FEATS.txt\", sep = \" \", index_col = 1)\n",
    "\n",
    "hla_seq_df = hla_full_pc_kf_pseudoseq_wseq.seq.copy()\n",
    "hla_seq_df_dict = {index: row.tolist() for index, row in pd.DataFrame(hla_seq_df).iterrows()}\n",
    "\n",
    "amino_acid_mapping = { #B = BOS token\n",
    "    'B': 0, 'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5,\n",
    "    'Q': 6, 'E': 7, 'G': 8, 'H': 9, 'I': 10,\n",
    "    'L': 11, 'K': 12, 'M': 13, 'F': 14, 'P': 15,\n",
    "    'S': 16, 'T': 17, 'W': 18, 'Y': 19, 'V': 20, \"-\":21\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe6d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(sequence, max_len, padding_token = \"-\"):\n",
    "    \"\"\"\n",
    "    Pads the amino acid sequence to a maximum length by adding the necessary number of padding tokens (\"-\") after the fourth amino acid\n",
    "\n",
    "    Parameters: \n",
    "    - sequence (str): input amino acid sequence for peptide\n",
    "    - max_len (int): maximum length you want to pad the sequence to \n",
    "    - padding_token (str): token used for padding\n",
    "    \n",
    "    Returns: \n",
    "    - str: padded amino acid sequence\n",
    "    \"\"\"\n",
    "    difference = max_len - len(sequence)\n",
    "    final_seq = sequence[:4] + padding_token*difference + sequence[4:]\n",
    "    return(final_seq)\n",
    "    \n",
    "def get_param(df, tgt_col, pep_col, hla_col):\n",
    "    y_data = df[tgt_col]\n",
    "    x_data = df[pep_col]\n",
    "    hla_name = df[hla_col]\n",
    "    max_len = max_len = max([len(x) for x in x_data])\n",
    "    return y_data, x_data, hla_name, max_len\n",
    "\n",
    "def initialize_param(model):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "class PositionalEncoding(nn.Module): #from https://pytorch.org/tutorials/beginner/transformer_tutorial.html#:~:text=A%20sequence%20of%20tokens%20are,TransformerEncoderLayer.\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1) #according to chatgpt, did not need to add \"device = device\" here since i have register_buffer \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58499179",
   "metadata": {},
   "outputs": [],
   "source": [
    "##0322: addded  esm_model, esm_batch_converter, esm_alphabet, here\n",
    "\n",
    "def transform_peptide_hla_sep(hla_seq, peptide_seq, max_len, esm_model, esm_batch_converter, esm_alphabet, encode_choice=\"embed\"):\n",
    "    padded_seq = add_padding(peptide_seq, max_len, \"-\")\n",
    "    \n",
    "    #converted this code to be for just one peptide since that is what the dataloader needs\n",
    "    if encode_choice == \"embed\":\n",
    "        peptide_processed = torch.tensor([amino_acid_mapping[aa] for aa in padded_seq])\n",
    "        hla_processed = torch.tensor([amino_acid_mapping[aa] for aa in hla_seq])\n",
    "        padding_mask = (peptide_processed != amino_acid_mapping[\"-\"]).unsqueeze(1)\n",
    "        \n",
    "    elif encode_choice == \"esm\":\n",
    "        padded_seq = add_padding(peptide_seq, max_len, \"<pad>\")\n",
    "        peptide_input = [(\"Peptide_Input\", padded_seq)]\n",
    "        hla_input = [(\"HLA_Input\", hla_seq)]\n",
    "        \n",
    "        peptide_batch_labels, peptide_batch_strs, peptide_batch_tokens = esm_batch_converter(peptide_input)\n",
    "        hla_batch_labels, hla_batch_strs, hla_batch_tokens = esm_batch_converter(hla_input)\n",
    "\n",
    "        # Extract per-residue representations (on CPU)\n",
    "        with torch.no_grad():\n",
    "            peptide_results = esm_model(peptide_batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "            hla_results = esm_model(hla_batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "        \n",
    "        peptide_processed = peptide_results[\"representations\"][33]\n",
    "        hla_processed = hla_results[\"representations\"][33]\n",
    "        \n",
    "        #this is the padding mask for the transformer (padding for the esm model has already been incorporated into the <pad> token for esm_model to deal with - double check! TODO)\n",
    "        padding_mask = (peptide_batch_tokens == esm_alphabet.padding_idx).unsqueeze(1) #QUESTION/TODO: Do I need to unsqueeze this? this is currently [batch_size, seq_length]\n",
    "        \n",
    "    elif encode_choice == \"one_hot\":\n",
    "        peptide_processed = torch.tensor([one_hot_dict[aa] for aa in padded_seq])\n",
    "        hla_processed = torch.tensor([one_hot_dict[aa] for aa in hla_seq])\n",
    "        padding_bool = [aa != \"-\" for aa in padded_seq]\n",
    "        padding_mask = torch.tensor(padding_bool).unsqueeze(1)\n",
    "\n",
    "    elif encode_choice == \"pc\":\n",
    "        peptide_processed = torch.tensor([pc_dict[aa] for aa in padded_seq])\n",
    "        hla_processed = torch.tensor([pc_dict[aa] for aa in hla_seq])\n",
    "        padding_bool = [aa != \"-\" for aa in padded_seq]\n",
    "        padding_mask = torch.tensor(padding_bool).unsqueeze(1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported encoding choice: {encode_choice}. Must be 'one_hot', 'esm', embed', or 'pc'\")\n",
    "    \n",
    "    return hla_processed, peptide_processed, padding_mask\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "#before using esm, need to define esm_model and esm_batch_converter: \n",
    "#esm_model, esm_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "#esm_batch_converter = esm_alphabet.get_batch_converter()\n",
    "#TODO: according to https://github.com/facebookresearch/esm/blob/main/esm/constants.py#L7, \"-\" is recognized by the model as <unk>, not <pad> so I might make that more explicit somehow --> TODO\n",
    "#NOTE: according to https://github.com/search?q=repo%3Afacebookresearch%2Fesm%20%3Cpad%3E&type=code, \"<pad>\" is used to get the padding_idx (and in the above, you see that it corresponds to 1 and the padded parts also correspond to 1 at the end of the seq)\n",
    "#NOTE: changed this such that aa == \"-\" is used for padding mask instead of aa != \"-\" since usually True means ignore the position and False means don't ignroe the position\n",
    "def transform_peptide_hla_concat(hla_seq, peptide_seq, max_len, esm_model, esm_batch_converter, esm_alphabet, encode_choice=\"embed\"):\n",
    "    padded_seq = add_padding(peptide_seq, max_len, \"-\")\n",
    "    concat_seq = padded_seq + hla_seq\n",
    "    \n",
    "    #TODO: for all of these, check that it should be unsqueeze(1)\n",
    "    #converted this code to be for just one peptide since that is what the dataloader needs\n",
    "    if encode_choice == \"embed\":\n",
    "        concat_processed = torch.tensor([amino_acid_mapping[aa] for aa in concat_seq])\n",
    "        padding_mask = (concat_processed == amino_acid_mapping[\"-\"]).unsqueeze(1)\n",
    "        \n",
    "    #TODO: check if this is correctly implemented for a one peptide at a time basis \n",
    "    elif encode_choice == \"esm\":\n",
    "        padded_seq = add_padding(peptide_seq, max_len, \"<pad>\")\n",
    "        concat_seq = padded_seq + hla_seq\n",
    "        concat_input = [(\"Concat_Input\", concat_seq)]\n",
    "        concat_batch_labels, concat_batch_strs, concat_batch_tokens = esm_batch_converter(concat_input)\n",
    "        #concat_batch_lens = (concat_batch_tokens != esm_alphabet.padding_idx).sum(1) #only need this if you want to later use the means instead of the token representations for each amino acid\n",
    "        \n",
    "        # Extract per-residue representations (on CPU)\n",
    "        with torch.no_grad():\n",
    "            concat_results = esm_model(concat_batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "        concat_processed = concat_results[\"representations\"][33]\n",
    "        #this is the padding mask for the transformer (padding for the esm model has already been incorporated into the <pad> token for esm_model to deal with - double check! TODO)\n",
    "        padding_mask = (concat_batch_tokens == esm_alphabet.padding_idx).unsqueeze(1) #QUESTION/TODO: Do I need to unsqueeze this? this is currently [batch_size, seq_length]\n",
    "\n",
    "    elif encode_choice == \"one_hot\":\n",
    "        concat_processed = torch.tensor([one_hot_dict[aa] for aa in concat_seq])\n",
    "        padding_bool = [aa == \"-\" for aa in concat_seq]\n",
    "        padding_mask = torch.tensor(padding_bool).unsqueeze(1)\n",
    "\n",
    "    elif encode_choice == \"pc\":\n",
    "        concat_processed = torch.tensor([pc_dict[aa] for aa in concat_seq])\n",
    "        padding_bool = [aa == \"-\" for aa in concat_seq]\n",
    "        padding_mask = torch.tensor(padding_bool).unsqueeze(1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported encoding choice: {encode_choice}. Must be 'one_hot', 'esm', embed', or 'pc'\")\n",
    "    \n",
    "    return concat_processed, padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe0d402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this if you want to create the train dataset automatically and can input the whole original file (no need to separate out the train and validation from each other beforehand)\n",
    "class PeptideHLADataset_Training_Validation(Dataset):\n",
    "    def __init__(self, input_df, hla_seq_df, tgt_col, pep_col, hla_col, fold_num_val, train_or_val, max_len, concat_or_sep, encode_choice = \"embed\", transform=None, esm_model=None, esm_batch_converter=None, esm_alphabet=None):\n",
    "        self.input_df = input_df\n",
    "        self.hla_seq_df = hla_seq_df\n",
    "        self.tgt_col = tgt_col\n",
    "        self.pep_col = pep_col\n",
    "        self.hla_col = hla_col\n",
    "        self.max_len = max_len\n",
    "        self.fold_num_val = fold_num_val\n",
    "        self.train_or_val = train_or_val\n",
    "        self.encode_choice = encode_choice\n",
    "        self.transform = transform\n",
    "        self.train_df = input_df[(input_df['fold'] == fold_num_val) & (input_df['set'] == \"train\")]\n",
    "        self.val_df = input_df[(input_df['fold'] == fold_num_val) & (input_df['set'] == \"valid\")]\n",
    "        #self.train_df = input_df[input_df['fold'] != fold_num_val]\n",
    "        #self.val_df = input_df[input_df['fold'] == fold_num_val]\n",
    "        self.esm_model = esm_model\n",
    "        self.esm_batch_converter = esm_batch_converter\n",
    "        self.esm_alphabet = esm_alphabet\n",
    "        self.concat_or_sep = concat_or_sep\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train_or_val == \"train\":\n",
    "            return len(self.train_df)\n",
    "        elif self.train_or_val == \"validation\":\n",
    "            return len(self.val_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(idx)\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        #print(idx)\n",
    "        if self.train_or_val == \"train\":\n",
    "            sample = self.train_df.iloc[idx]\n",
    "        elif self.train_or_val == \"validation\":\n",
    "            sample = self.val_df.iloc[idx]\n",
    "        #print(sample.shape)   \n",
    "        peptide = sample[self.pep_col]\n",
    "        hla_name = sample[self.hla_col]\n",
    "        label = sample[self.tgt_col]\n",
    "        hla_sequence = self.hla_seq_df[hla_name]\n",
    "\n",
    "        if self.transform:\n",
    "            if self.concat_or_sep == \"concat\":\n",
    "                concat_processed, padding_mask = self.transform(hla_sequence, peptide, self.max_len, self.esm_model, self.esm_batch_converter, self.esm_alphabet, self.encode_choice)\n",
    "                #TODO/CHECK: added this to get rid of the extra dim at the front \n",
    "                #return concat_processed, padding_mask, label #original - use w a custom collate fn\n",
    "                #print(\"dataset\")\n",
    "                #print(concat_processed.squeeze(0).shape)\n",
    "                #print(padding_mask.shape)\n",
    "                return concat_processed.squeeze(0), padding_mask, label\n",
    "            \n",
    "            elif self.concat_or_sep == \"sep\":\n",
    "                hla_sequence, peptide, padding_mask = self.transform(hla_sequence, peptide, self.max_len, self.esm_model, self.esm_batch_converter, self.esm_alphabet, self.encode_choice)\n",
    "                #return hla_sequence, peptide, padding_mask, label #original - use w a custom collate fn\n",
    "                return hla_sequence.squeeze(0), peptide.squeeze(0), padding_mask, label\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"'concat_or_sep' parameter must be 'concat' or 'sep'.\")\n",
    "        else: \n",
    "            raise RuntimeError(\"Transform function is not defined. Please define it.\")\n",
    "\n",
    "def get_data_loaders(input_df, hla_seq_df, tgt_col, pep_col, hla_col, fold_num_val, max_len, esm_model=None, esm_batch_converter=None, esm_alphabet=None, batch_size = 32, encode_choice = 'embed', concat_or_sep = \"concat\", collate_fn=None):\n",
    "    if concat_or_sep == \"concat\":\n",
    "        transform_function = transform_peptide_hla_concat\n",
    "    elif concat_or_sep == \"sep\":\n",
    "        transform_function = transform_peptide_hla_sep\n",
    "    else:\n",
    "        raise ValueError(\"concat_or_sep argument must be either 'concat' or 'sep'\")\n",
    "        \n",
    "    train_dataset = PeptideHLADataset_Training_Validation(input_df, hla_seq_df, tgt_col, pep_col, hla_col, fold_num_val, \"train\", max_len = max_len, transform = transform_function, concat_or_sep = concat_or_sep, encode_choice = encode_choice, esm_model=esm_model, esm_batch_converter=esm_batch_converter, esm_alphabet=esm_alphabet)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = PeptideHLADataset_Training_Validation(input_df, hla_seq_df, tgt_col, pep_col, hla_col, fold_num_val, \"validation\", max_len = max_len, transform = transform_function, concat_or_sep = concat_or_sep, encode_choice = encode_choice, esm_model=esm_model, esm_batch_converter=esm_batch_converter, esm_alphabet=esm_alphabet)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de4f0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, model_type, optimizer, criterion, epoch, concat_or_sep, pooling_strat):\n",
    "    model.train()\n",
    "    #print(1)\n",
    "    #print(pooling_strat)\n",
    "    running_loss = 0.0\n",
    "    if concat_or_sep == \"concat\":\n",
    "        #print(\"training start\")\n",
    "        for batch_idx, (concat_processed, padding_mask, labels) in enumerate(train_loader):\n",
    "            #added this for gpu usage: \n",
    "            concat_processed, padding_mask, labels = concat_processed.to(device), padding_mask.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #outputs = model(concat_processed.float(), model_type = model_type, src_key_padding_mask = padding_mask).squeeze() #(reduce a dim so it matches the dim of the labels [32] instead of [32, 1])            \n",
    "            #outputs = model(concat_processed.float(), pooling_strat = pooling_strat, model_type = model_type, src_key_padding_mask = padding_mask).squeeze() #(reduce a dim so it matches the dim of the labels [32] instead of [32, 1])        \n",
    "            #outputs = model(concat_processed.float(), pooling_strat, padding_mask, model_type).squeeze() #(reduce a dim so it matches the dim of the labels [32] instead of [32, 1])            \n",
    "            outputs = model(concat_processed.float(), pooling_strat = pooling_strat, model_type = model_type, src_key_padding_mask = padding_mask.squeeze(1).squeeze(1)).squeeze() #(reduce a dim so it matches the dim of the labels [32] instead of [32, 1])        \n",
    "            loss = criterion(outputs, labels.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 100 == 99:  #Print average loss every 10 mini-batches\n",
    "                print(f'Training Epoch: {epoch + 1}, Batch: {batch_idx + 1}, Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "        #print(\"training done\")\n",
    "    elif concat_or_sep == \"sep\":\n",
    "        #print(2)\n",
    "        #print(pooling_strat)\n",
    "        for batch_idx, (hla_processed, peptide_processed, padding_mask, labels) in enumerate(train_loader):\n",
    "            #added this for gpu usage: \n",
    "            hla_processed, peptide_processed, padding_mask, labels = hla_processed.to(device), peptide_processed.to(device), padding_mask.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #outputs = model(hla_processed.long(), peptide_processed.long(), pooling_strat = pooling_strat, model_type = model_type, src_key_padding_mask = padding_mask).squeeze() #(reduce a dim so it matches the dim of the labels [32] instead of [32, 1])\n",
    "            #outputs = model(hla_processed.long(), peptide_processed.long(), pooling_strat, padding_mask, model_type).squeeze()\n",
    "            outputs = model(hla_processed.float(), peptide_processed.float(), pooling_strat = pooling_strat, model_type = model_type, src_key_padding_mask = padding_mask.squeeze(1).squeeze(1)).squeeze() #(reduce a dim so it matches the dim of the labels [32] instead of [32, 1])\n",
    "            loss = criterion(outputs, labels.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 100 == 99:  #Print average loss every 10 mini-batches\n",
    "                print(f'Training Epoch: {epoch + 1}, Batch: {batch_idx + 1}, Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "    else:\n",
    "        raise ValueError(\"concat_or_sep argument must be either 'concat' or 'sep'\")\n",
    "    \n",
    "'''def eval_model(model, val_loader, model_type, threshold, criterion, concat_or_sep, pooling_strat):\n",
    "    model.eval()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    \n",
    "    if concat_or_sep == \"concat\":\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (concat_processed, padding_mask, labels) in enumerate(val_loader):\n",
    "                #IMPT: only have the squeeze here if you are doing binary classification w sigmoid classification! if you use softmax for 2 classes, take this out\n",
    "                #outputs = model(concat_processed.long(), pooling_strat = pooling_strat, model_type = model_type, src_key_padding_mask = padding_mask).squeeze()\n",
    "                #outputs = model(concat_processed.long(), pooling_strat, padding_mask, model_type).squeeze()\n",
    "                outputs = model(concat_processed.float(), pooling_strat = pooling_strat, model_type = model_type, src_key_padding_mask = padding_mask.squeeze(1).squeeze(1)).squeeze() #(reduce a dim so it matches the dim of the labels [32] instead of [32, 1])        \n",
    "                #predicted = torch.argmax(output, dim=1) #for softmax\n",
    "                predicted = (outputs >= threshold).long() #for sigmoid\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "    elif concat_or_sep == \"sep\":\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (hla_processed, peptide_processed, padding_mask, labels) in enumerate(val_loader):\n",
    "                #IMPT: only have the squeeze here if you are doing binary classification w sigmoid classification! if you use softmax for 2 classes, take this out\n",
    "                #outputs = model(hla_processed.long(), peptide_processed.long(), pooling_strat = pooling_strat, model_type = model_type, src_key_padding_mask = padding_mask).squeeze()\n",
    "                #outputs = model(hla_processed.long(), peptide_processed.long(), pooling_strat, padding_mask, model_type).squeeze()\n",
    "                outputs = model(hla_processed.float(), peptide_processed.float(), pooling_strat = pooling_strat, model_type = model_type, src_key_padding_mask = padding_mask.squeeze(1).squeeze(1)).squeeze()\n",
    "                \n",
    "                #predicted = torch.argmax(output, dim=1) #for softmax\n",
    "                predicted = (outputs >= threshold).long() #for sigmoid\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "    else:\n",
    "        raise ValueError(\"concat_or_sep argument must be either 'concat' or 'sep'\")\n",
    "            \n",
    "    accuracy = total_correct / total_samples\n",
    "    return accuracy'''\n",
    "\n",
    "def eval_model(model, val_loader, model_type, threshold, criterion, concat_or_sep, pooling_strat):\n",
    "    model.eval()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, padding_mask, labels) in enumerate(val_loader):\n",
    "            inputs, labels = inputs.to(device), padding_mask.to(device), labels.to(device)\n",
    "            if concat_or_sep == \"concat\":\n",
    "                outputs = model(inputs.float(), pooling_strat=pooling_strat, model_type=model_type, src_key_padding_mask=padding_mask.squeeze(1).squeeze(1))\n",
    "            elif concat_or_sep == \"sep\":\n",
    "                hla_processed, peptide_processed = inputs\n",
    "                outputs = model(hla_processed.float(), peptide_processed.float(), pooling_strat=pooling_strat, model_type=model_type, src_key_padding_mask=padding_mask.squeeze(1).squeeze(1))\n",
    "            else:\n",
    "                raise ValueError(\"concat_or_sep argument must be either 'concat' or 'sep'\")\n",
    "            \n",
    "            outputs = outputs.squeeze()\n",
    "            predicted = (outputs >= threshold).long()\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "            \n",
    "            if batch_idx % 100 == 99:\n",
    "                print(f'Validation Epoch: {epoch + 1}, Validation Metrics - Accuracy: {accuracy:.4f}, MSE: {mse:.4f}, AUC: {auc:.4f}, PR AUC: {pr_auc:.4f}')\n",
    "                \n",
    "    accuracy = total_correct / total_samples\n",
    "    mse = mse_loss(torch.tensor(all_outputs), torch.tensor(all_labels).float()).item()\n",
    "    auc = roc_auc_score(all_labels, all_outputs)\n",
    "    pr_auc = average_precision_score(all_labels, all_outputs)\n",
    "    \n",
    "    return accuracy, mse, auc, pr_auc\n",
    "\n",
    "def train_and_eval(model, train_loader, val_loader, model_type, threshold, criterion, optimizer, pooling_strat, epochs=10, concat_or_sep = \"concat\"):\n",
    "    #print(5)\n",
    "    #print(pooling_strat)\n",
    "    if optimizer == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    for epoch in range(epochs):\n",
    "        # training\n",
    "        train_model(model, train_loader, model_type, optimizer, criterion, epoch, concat_or_sep, pooling_strat)\n",
    "        # eval\n",
    "        accuracy, mse, auc, pr_auc = eval_model(model, val_loader, model_type, threshold, criterion, concat_or_sep, pooling_strat)\n",
    "        #print(f'Eval Epoch: {epoch + 1}, Validation Accuracy: {accuracy:.4f}')\n",
    "        #print(f'Eval Epoch: {epoch + 1}, Validation Metrics - Accuracy: {accuracy:.4f}, MSE: {mse:.4f}, AUC: {auc:.4f}, PR AUC: {pr_auc:.4f}')\n",
    "\n",
    "    print('Finished Training and Evaluation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12003c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model, test_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "test_batch_converter = test_alphabet.get_batch_converter()\n",
    "\n",
    "max_len = max([len(x) for x in peptide_10mer_subset.seq])\n",
    "input_dim = 1280\n",
    "num_layers = 6 #can change\n",
    "num_heads = 8 #can change\n",
    "d_ff = 2048\n",
    "criterion = nn.BCELoss()\n",
    "encode_choice = \"esm\"\n",
    "fold_values = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab52f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f5a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderClassifier_Model1(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(ntoken, d_model) #not used in esm model!\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.linear = nn.Linear(d_model, 1) #bring it back to dim 1 for sigmoid classifier (if using softmax, change this to 2)\n",
    "\n",
    "    def forward(self, src, pooling_strat = \"CLS\", model_type = \"esm\", src_mask=None, src_key_padding_mask=None) -> Tensor:\n",
    "        #change src (input) shape from [batch_size, seq_len, input_dim] to [seq_len, batch_size, input_dim] (latter is more common for pytorch built-in implementations)\n",
    "        #src = src.permute(1, 0, 2) -- added batch_first=True so do not need this anymore\n",
    "        \n",
    "        if model_type == \"esm\": \n",
    "            #src = self.embedding(src) #add this back in when you generalize the model for other types\n",
    "            src = self.pos_encoder(src)\n",
    "        '''else:\n",
    "            src = self.embedding(src) #add this back in when you generalize the model for other types\n",
    "            src = self.pos_encoder(src)'''\n",
    "        \n",
    "        #run model\n",
    "        #print(\"model\")\n",
    "        #print(src.shape)\n",
    "        encoded_interim = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
    "        #print(encoded_interim.shape)\n",
    "        \n",
    "        ## Three ways to do this: take the representation of the [CLS] token, take the mean pooling strategy, \n",
    "        # or take the raw values themselves (unsure if the last one is actually used in practice!!)\n",
    "        if pooling_strat == \"mean\":\n",
    "            #shape of encoded_interim is [batch_size, seq_len, features]\n",
    "            encoded = encoded_interim.mean(dim=1) #changed dim = 0 to dim = 1 since that is now the seq_len dim\n",
    "        elif pooling_strat == \"CLS\":\n",
    "            #assuming CLS is the first token at position 0 (assuming it is the same as SOS token for our purposes - check with Wengong [TODO])\n",
    "            # With batch_first=True, the CLS token for each sequence is at position 0 along the seq_len dimension, but since we're iterating over batch, index over each item in the batch.\n",
    "            #shape of encoded_interim is [batch_size, seq_len, features]\n",
    "            encoded = encoded_interim[:, 0, :]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid pooling strategy: {pooling_strat}\")\n",
    "        #run classifier (bring it down to one neuron then use sigmoid classifier)\n",
    "        logits = self.linear(encoded)\n",
    "        return torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c201f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "Training Epoch: 1, Batch: 100, Loss: 9.4764\n",
      "Training Epoch: 1, Batch: 200, Loss: 7.2388\n",
      "Training Epoch: 1, Batch: 300, Loss: 7.4418\n",
      "Training Epoch: 1, Batch: 400, Loss: 7.1791\n",
      "Training Epoch: 1, Batch: 600, Loss: 7.0588\n",
      "Training Epoch: 1, Batch: 800, Loss: 7.0333\n",
      "Training Epoch: 1, Batch: 900, Loss: 7.1468\n",
      "Training Epoch: 1, Batch: 1000, Loss: 7.0358\n",
      "Training Epoch: 1, Batch: 1100, Loss: 7.0375\n",
      "Training Epoch: 1, Batch: 1200, Loss: 7.0005\n",
      "Training Epoch: 1, Batch: 1300, Loss: 7.0351\n",
      "Training Epoch: 1, Batch: 1400, Loss: 7.0085\n",
      "Training Epoch: 1, Batch: 1500, Loss: 7.0794\n",
      "Training Epoch: 1, Batch: 1600, Loss: 7.0090\n",
      "Training Epoch: 1, Batch: 1700, Loss: 6.9969\n",
      "Training Epoch: 1, Batch: 1800, Loss: 7.0651\n",
      "Training Epoch: 1, Batch: 1900, Loss: 6.9875\n",
      "Training Epoch: 1, Batch: 2000, Loss: 7.0027\n",
      "Training Epoch: 1, Batch: 2100, Loss: 6.9776\n",
      "Training Epoch: 1, Batch: 2200, Loss: 6.9832\n",
      "Training Epoch: 1, Batch: 2300, Loss: 7.0155\n",
      "Training Epoch: 1, Batch: 2400, Loss: 6.9611\n",
      "Training Epoch: 1, Batch: 2500, Loss: 6.9899\n",
      "Training Epoch: 1, Batch: 2600, Loss: 7.0131\n",
      "Training Epoch: 1, Batch: 2700, Loss: 7.0356\n",
      "Training Epoch: 1, Batch: 2800, Loss: 6.9921\n",
      "Training Epoch: 1, Batch: 2900, Loss: 6.9818\n",
      "Training Epoch: 1, Batch: 3000, Loss: 6.9748\n",
      "Training Epoch: 1, Batch: 3100, Loss: 6.9551\n",
      "Training Epoch: 1, Batch: 3200, Loss: 7.0192\n",
      "Training Epoch: 1, Batch: 3300, Loss: 6.9548\n",
      "Training Epoch: 1, Batch: 3400, Loss: 6.9692\n",
      "Training Epoch: 1, Batch: 3500, Loss: 7.0049\n",
      "Training Epoch: 1, Batch: 3600, Loss: 7.0116\n",
      "Training Epoch: 1, Batch: 3700, Loss: 6.9742\n",
      "Training Epoch: 1, Batch: 3800, Loss: 7.0107\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 1\")\n",
    "for val_fold_num in fold_values:\n",
    "    model = TransformerEncoderClassifier_Model1(ntoken = 22, \n",
    "                                            d_model = input_dim, \n",
    "                                            nhead = num_heads, \n",
    "                                            d_hid = d_ff, \n",
    "                                            nlayers = num_layers, \n",
    "                                            dropout = 0.1).to(device)\n",
    "    initialize_param(model)\n",
    "    train_loader, val_loader = get_data_loaders(peptide_10mer_subset, hla_seq_df, \"label\", \"seq\", \"allele\", val_fold_num, max_len, esm_model = test_model, esm_batch_converter = test_batch_converter, esm_alphabet = test_alphabet, batch_size = 32, encode_choice = encode_choice, concat_or_sep = \"concat\")\n",
    "    train_and_eval(\n",
    "        model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        model_type=\"esm\", \n",
    "        threshold=0.5, \n",
    "        criterion=criterion, \n",
    "        optimizer=\"Adam\",\n",
    "        pooling_strat = \"CLS\",\n",
    "        epochs=10, \n",
    "        concat_or_sep=\"concat\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36cec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "125018e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderClassifier_Model2(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.1, fine_tune_dim1: int = 512, fine_tune_dim2: int = 512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(ntoken, d_model) #not used in esm model!\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Adding 3 linear layers for fine-tuning ESM embeddings\n",
    "        self.linear1 = nn.Linear(input_dim, fine_tune_dim1)\n",
    "        self.linear2 = nn.Linear(fine_tune_dim1, fine_tune_dim2)\n",
    "        self.linear3 = nn.Linear(fine_tune_dim2, input_dim)\n",
    "        \n",
    "        # Add dropout since you added the linear layers \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Variety of activation functions to choose from since we added 3 linear layers \n",
    "        self.relu = nn.ReLU()\n",
    "        self.swish = nn.SiLU()\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.elu = nn.ELU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.linear = nn.Linear(d_model, 1) #bring it back to dim 1 for sigmoid classifier (if using softmax, change this to 2)\n",
    "\n",
    "    def forward(self, src, pooling_strat = \"CLS\", model_type = \"esm\", activ_fn = \"ReLU\", src_mask=None, src_key_padding_mask=None):\n",
    "        #print(0)\n",
    "        #print(pooling_strat)\n",
    "        \n",
    "        if activ_fn == \"ReLU\":\n",
    "            src = self.dropout(self.relu(self.linear1(src)))\n",
    "            src = self.dropout(self.relu(self.linear2(src)))\n",
    "            src = self.dropout(self.relu(self.linear3(src)))\n",
    "        elif activ_fn == \"Swish\":\n",
    "            src = self.dropout(self.swish(self.linear1(src)))\n",
    "            src = self.dropout(self.swish(self.linear2(src)))\n",
    "            src = self.dropout(self.swish(self.linear3(src)))\n",
    "        elif activ_fn == \"LeakyReLU\":\n",
    "            src = self.dropout(self.leakyrelu(self.linear1(src)))\n",
    "            src = self.dropout(self.leakyrelu(self.linear2(src)))\n",
    "            src = self.dropout(self.leakyrelu(self.linear3(src)))\n",
    "        elif activ_fn == \"PReLU\":\n",
    "            src = self.dropout(self.prelu(self.linear1(src)))\n",
    "            src = self.dropout(self.prelu(self.linear2(src)))\n",
    "            src = self.dropout(self.prelu(self.linear3(src)))\n",
    "        elif activ_fn == \"ELU\":\n",
    "            src = self.dropout(self.elu(self.linear1(src)))\n",
    "            src = self.dropout(self.elu(self.linear2(src)))\n",
    "            src = self.dropout(self.elu(self.linear3(src)))\n",
    "        elif activ_fn == \"SELU\":\n",
    "            src = self.dropout(self.selu(self.linear1(src)))\n",
    "            src = self.dropout(self.selu(self.linear2(src)))\n",
    "            src = self.dropout(self.selu(self.linear3(src)))\n",
    "        elif activ_fn == \"GELU\":\n",
    "            src = self.dropout(self.gelu(self.linear1(src)))\n",
    "            src = self.dropout(self.gelu(self.linear2(src)))\n",
    "            src = self.dropout(self.gelu(self.linear3(src)))\n",
    "        \n",
    "        if model_type == \"esm\": \n",
    "            #src = self.embedding(src) #add this back in when you generalize the model for other types\n",
    "            src = self.pos_encoder(src)\n",
    "        else:\n",
    "            #print(model_type)\n",
    "            #print(\"incorrect\")\n",
    "            src = self.embedding(src) #add this back in when you generalize the model for other types\n",
    "            src = self.pos_encoder(src)\n",
    "            \n",
    "        #run model\n",
    "        encoded_interim = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        ## Three ways to do this: take the representation of the [CLS] token, take the mean pooling strategy, \n",
    "        # or take the raw values themselves (unsure if the last one is actually used in practice!!)\n",
    "        if pooling_strat == \"mean\":\n",
    "            encoded = encoded_interim.mean(dim=1) #changed dim = 0 to dim = 1 since that is now the seq_len dim\n",
    "        elif pooling_strat == \"CLS\":\n",
    "            #assuming CLS is the first token at position 0 (assuming it is the same as SOS token for our purposes - check with Wengong [TODO])\n",
    "            # With batch_first=True, the CLS token for each sequence is at position 0 along the seq_len dimension, but since we're iterating over batch, index over each item in the batch.\n",
    "            encoded = encoded_interim[:, 0, :]\n",
    "        #run classifier (bring it down to one neuron then use sigmoid classifier)\n",
    "        logits = self.linear(encoded)\n",
    "        return torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8291bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2\n",
      "Eval Epoch: 1, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.6000\n",
      "Finished Training and Evaluation\n",
      "Eval Epoch: 1, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.6000\n",
      "Finished Training and Evaluation\n",
      "Eval Epoch: 1, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.6000\n",
      "Finished Training and Evaluation\n",
      "Eval Epoch: 1, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.6000\n",
      "Finished Training and Evaluation\n",
      "Eval Epoch: 1, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.6000\n",
      "Finished Training and Evaluation\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 2\")\n",
    "for val_fold_num in fold_values:\n",
    "    model = TransformerEncoderClassifier_Model2(ntoken = 22, \n",
    "                                            d_model = input_dim, \n",
    "                                            nhead = num_heads, \n",
    "                                            d_hid = d_ff, \n",
    "                                            nlayers = num_layers, \n",
    "                                            dropout = 0.1,\n",
    "                                            fine_tune_dim1 = 512, \n",
    "                                            fine_tune_dim2 = 512).to(device)\n",
    "    initialize_param(model)\n",
    "    train_loader, val_loader = get_data_loaders(peptide_10mer_subset, hla_seq_df, \"label\", \"seq\", \"allele\", val_fold_num, max_len, esm_model = test_model, esm_batch_converter = test_batch_converter, esm_alphabet = test_alphabet, batch_size = 32, encode_choice = encode_choice, concat_or_sep = \"concat\")\n",
    "    train_and_eval(\n",
    "        model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        model_type=\"esm\", \n",
    "        threshold=0.5, \n",
    "        criterion=criterion, \n",
    "        optimizer=\"Adam\",\n",
    "        pooling_strat = \"CLS\",\n",
    "        epochs=5, \n",
    "        concat_or_sep=\"concat\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "072976e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18b3d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderClassifier_Model4(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(ntoken, d_model) #not used in esm model!\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True) # batch_first=True for consistency\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.linear = nn.Linear(d_model, 1) #bring it back to dim 1 for sigmoid classifier (if using softmax, change this to 2)\n",
    "\n",
    "    def forward(self, hla, peptide, pooling_strat = \"CLS\", model_type = \"esm\", src_mask=None, src_key_padding_mask=None) -> Tensor:\n",
    "        #positional encoding\n",
    "        if model_type == \"esm\": \n",
    "            #positionally encode them separately\n",
    "            hla = self.pos_encoder(hla)\n",
    "            peptide = self.pos_encoder(peptide)\n",
    "            #print(hla.shape)\n",
    "            #print(peptide.shape)\n",
    "            #print(src_key_padding_mask.shape)\n",
    "        else:\n",
    "            hla = self.embedding(hla) #add this back in when you generalize the model for other types\n",
    "            hla = self.pos_encoder(hla)\n",
    "            peptide = self.embedding(peptide)\n",
    "            peptide = self.pos_encoder(peptide)\n",
    "\n",
    "        #cross-attention: hla as query, peptide as key and value\n",
    "        #https://stackoverflow.com/questions/62629644/what-the-difference-between-att-mask-and-key-padding-mask-in-multiheadattnetion\n",
    "        attn_output, attn_output_weights = self.multihead_attn(query=hla, key=peptide, value=peptide, key_padding_mask=src_key_padding_mask)\n",
    "        #attn_output, attn_output_weights = self.multihead_attn(query=peptide, key=hla, value=hla, key_padding_mask=peptide_key_padding_mask)\n",
    "        \n",
    "        #print(attn_output.shape)\n",
    "        \n",
    "        ##encoder - CHECK/TODO: I am directly passing attn_output to the encoder but there might be other ways to do this like adding \n",
    "        #a residual layer where I add back the peptide and hla embeddings back in as well to prevent loss of details\n",
    "        ##steps taken - CHECK/TODO: compute cross-attention between HLA and peptide sequences, where output represents the peptide seq in the context of the HLA seq. Feed the cross-attention output into the encoder stack. \n",
    "        #encoder layers will perform self-attention on the already combined HLA-peptide context rather than the original separate sequences --> is that okay?\n",
    "        \n",
    "        #TODO: ask wengong: what is the mask supposed to be here? It is supposed to cover the peptides only, but 1) that was masked in the multihead attention above and 2)the dim here are of the hla only so idk what to mask anymore in this combined representation that is the attn_output\n",
    "        encoded_interim = self.transformer_encoder(attn_output, src_key_padding_mask=None)\n",
    "    \n",
    "        if pooling_strat == \"mean\":\n",
    "            encoded = encoded_interim.mean(dim=1)\n",
    "        elif pooling_strat == \"CLS\":\n",
    "            #assuming CLS is the first token at position 0 (assuming it is the same as SOS token for our purposes - check with Wengong [TODO])\n",
    "            encoded = encoded_interim[:, 0, :]\n",
    "        #run classifier\n",
    "        logits = self.linear(encoded)\n",
    "        return torch.sigmoid(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36ac63ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4\n",
      "Eval Epoch: 1, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.6000\n",
      "Finished Training and Evaluation\n",
      "Eval Epoch: 1, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.4000\n",
      "Finished Training and Evaluation\n",
      "Eval Epoch: 1, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.6000\n",
      "Finished Training and Evaluation\n",
      "Eval Epoch: 1, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.6000\n",
      "Finished Training and Evaluation\n",
      "Eval Epoch: 1, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.6000\n",
      "Finished Training and Evaluation\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 4\")\n",
    "for val_fold_num in fold_values:\n",
    "    model = TransformerEncoderClassifier_Model4(ntoken = 22, \n",
    "                                            d_model = input_dim, \n",
    "                                            nhead = num_heads, \n",
    "                                            d_hid = d_ff, \n",
    "                                            nlayers = num_layers, \n",
    "                                            dropout = 0.1).to(device)\n",
    "    initialize_param(model)\n",
    "    train_loader, val_loader = get_data_loaders(peptide_10mer_subset, hla_seq_df, \"label\", \"seq\", \"allele\", val_fold_num, max_len, esm_model = test_model, esm_batch_converter = test_batch_converter, esm_alphabet = test_alphabet, batch_size = 32, encode_choice = encode_choice, concat_or_sep = \"sep\")\n",
    "    train_and_eval(\n",
    "        model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        model_type=\"esm\", \n",
    "        threshold=0.5, \n",
    "        criterion=criterion, \n",
    "        optimizer=\"Adam\",\n",
    "        pooling_strat = \"CLS\",\n",
    "        epochs=5, \n",
    "        concat_or_sep=\"sep\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "823da014",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6943932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderClassifier_Model5(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.1, fine_tune_dim1: int = 512, fine_tune_dim2: int = 512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(ntoken, d_model) #not used in esm model!\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "         \n",
    "        # Adding 3 linear layers for fine-tuning ESM embeddings\n",
    "        self.linear1_pep = nn.Linear(input_dim, fine_tune_dim1)\n",
    "        self.linear2_pep = nn.Linear(fine_tune_dim1, fine_tune_dim2)\n",
    "        self.linear3_pep = nn.Linear(fine_tune_dim2, input_dim)\n",
    "        \n",
    "        self.linear1_hla = nn.Linear(input_dim, fine_tune_dim1)\n",
    "        self.linear2_hla = nn.Linear(fine_tune_dim1, fine_tune_dim2)\n",
    "        self.linear3_hla = nn.Linear(fine_tune_dim2, input_dim)\n",
    "        \n",
    "        # Add dropout since you added the linear layers \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Variety of activation functions to choose from since we added 3 linear layers \n",
    "        self.relu = nn.ReLU()\n",
    "        self.swish = nn.SiLU()\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.elu = nn.ELU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True) # batch_first=True for consistency\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.linear = nn.Linear(d_model, 1) #bring it back to dim 1 for sigmoid classifier (if using softmax, change this to 2)\n",
    "\n",
    "    def forward(self, hla, peptide, pooling_strat=\"CLS\", model_type=\"esm\", activ_fn = \"ReLU\", src_mask=None, src_key_padding_mask=None) -> Tensor:\n",
    "        \n",
    "        #run the three linear layers SEPARATELY for each hla or peptide \n",
    "        if activ_fn == \"ReLU\":\n",
    "            activation_fn = self.relu\n",
    "        elif activ_fn == \"Swish\":\n",
    "            activation_fn = self.swish\n",
    "        elif activ_fn == \"LeakyReLU\":\n",
    "            activation_fn = self.leakyrelu\n",
    "        elif activ_fn == \"PReLU\":\n",
    "            activation_fn = self.prelu\n",
    "        elif activ_fn == \"ELU\":\n",
    "            activation_fn = self.elu\n",
    "        elif activ_fn == \"SELU\":\n",
    "            activation_fn = self.selu\n",
    "        elif activ_fn == \"GELU\":\n",
    "            activation_fn = self.gelu\n",
    "            \n",
    "        hla = self.dropout(activation_fn(self.linear1_hla(hla)))\n",
    "        hla = self.dropout(activation_fn(self.linear2_hla(hla)))\n",
    "        hla = self.dropout(activation_fn(self.linear3_hla(hla)))\n",
    "        \n",
    "        peptide = self.dropout(activation_fn(self.linear1_pep(peptide)))\n",
    "        peptide = self.dropout(activation_fn(self.linear2_pep(peptide)))\n",
    "        peptide = self.dropout(activation_fn(self.linear3_pep(peptide)))\n",
    "            \n",
    "        #positional encoding\n",
    "        if model_type == \"esm\": \n",
    "            #positionally encode them separately\n",
    "            hla = self.pos_encoder(hla)\n",
    "            peptide = self.pos_encoder(peptide)\n",
    "            #print(hla.shape)\n",
    "            #print(peptide.shape)\n",
    "            #print(src_key_padding_mask.shape)\n",
    "        else:\n",
    "            hla = self.embedding(hla) #add this back in when you generalize the model for other types\n",
    "            hla = self.pos_encoder(hla)\n",
    "            peptide = self.embedding(peptide)\n",
    "            peptide = self.pos_encoder(peptide)\n",
    "\n",
    "        #cross-attention: hla as query, peptide as key and value\n",
    "        #https://stackoverflow.com/questions/62629644/what-the-difference-between-att-mask-and-key-padding-mask-in-multiheadattnetion\n",
    "        attn_output, attn_output_weights = self.multihead_attn(query=hla, key=peptide, value=peptide, key_padding_mask=src_key_padding_mask)\n",
    "        #attn_output, attn_output_weights = self.multihead_attn(query=peptide, key=hla, value=hla, key_padding_mask=peptide_key_padding_mask)\n",
    "\n",
    "        ##encoder - CHECK/TODO: I am directly passing attn_output to the encoder but there might be other ways to do this like adding \n",
    "        #a residual layer where I add back the peptide and hla embeddings back in as well to prevent loss of details\n",
    "        ##steps taken - CHECK/TODO: compute cross-attention between HLA and peptide sequences, where output represents the peptide seq in the context of the HLA seq. Feed the cross-attention output into the encoder stack. \n",
    "        #encoder layers will perform self-attention on the already combined HLA-peptide context rather than the original separate sequences --> is that okay?\n",
    "        #TODO: ask wengong: what is the mask supposed to be here? It is supposed to cover the peptides only, but 1) that was masked in the multihead attention above and 2)the dim here are of the hla only so idk what to mask anymore in this combined representation that is the attn_output\n",
    "        encoded_interim = self.transformer_encoder(attn_output, src_key_padding_mask=None)\n",
    "    \n",
    "        if pooling_strat == \"mean\":\n",
    "            encoded = encoded_interim.mean(dim=1)\n",
    "        elif pooling_strat == \"CLS\":\n",
    "            #assuming CLS is the first token at position 0 (assuming it is the same as SOS token for our purposes - check with Wengong [TODO])\n",
    "            encoded = encoded_interim[:, 0, :]\n",
    "        #run classifier\n",
    "        logits = self.linear(encoded)\n",
    "        return torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91840eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 5\n",
      "Eval Epoch: 1, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.6000\n",
      "Finished Training and Evaluation\n",
      "Eval Epoch: 1, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.6000\n",
      "Finished Training and Evaluation\n",
      "Eval Epoch: 1, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.4000\n",
      "Finished Training and Evaluation\n",
      "Eval Epoch: 1, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.4000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.6000\n",
      "Finished Training and Evaluation\n",
      "Eval Epoch: 1, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 2, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 3, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 4, Validation Accuracy: 0.6000\n",
      "Eval Epoch: 5, Validation Accuracy: 0.6000\n",
      "Finished Training and Evaluation\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 5\")\n",
    "for val_fold_num in fold_values:\n",
    "    model = TransformerEncoderClassifier_Model5(ntoken = 22, \n",
    "                                            d_model = input_dim, \n",
    "                                            nhead = num_heads, \n",
    "                                            d_hid = d_ff, \n",
    "                                            nlayers = num_layers, \n",
    "                                            dropout = 0.1,\n",
    "                                            fine_tune_dim1 = 512, \n",
    "                                            fine_tune_dim2 = 512).to(device)\n",
    "    initialize_param(model)\n",
    "    train_loader, val_loader = get_data_loaders(peptide_10mer_subset, hla_seq_df, \"label\", \"seq\", \"allele\", val_fold_num, max_len, esm_model = test_model, esm_batch_converter = test_batch_converter, esm_alphabet = test_alphabet, batch_size = 32, encode_choice = encode_choice, concat_or_sep = \"sep\")\n",
    "    train_and_eval(\n",
    "        model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        model_type=\"esm\", \n",
    "        threshold=0.5, \n",
    "        criterion=criterion, \n",
    "        optimizer=\"Adam\",\n",
    "        pooling_strat = \"CLS\",\n",
    "        epochs=5, \n",
    "        concat_or_sep=\"sep\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109581d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea091ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
